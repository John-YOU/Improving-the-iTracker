{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import caffe\n",
    "import dlib\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import cv2\n",
    "from sklearn import *\n",
    "from numpy import *\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CaffeNet found.\n"
     ]
    }
   ],
   "source": [
    "caffe_root='/home/gtx1080ti/Documents/hanyou2/gazecapture/'\n",
    "#sys.path.insert(0, caffe_root + 'python')\n",
    "\n",
    "if os.path.isfile(caffe_root + 'snapshots/itracker25x_iter_92000.caffemodel'):\n",
    "    print('CaffeNet found.')\n",
    "else:\n",
    "    print('Downloading pre-trained CaffeNet model...')\n",
    "    #!../scripts/download_model_binary.py ../models/bvlc_reference_caffenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the path of dataset\n",
    "name='0206002' \n",
    "home='/home/gtx1080ti/Documents/hanyou2/0206/'\n",
    "path='/home/gtx1080ti/Documents/hanyou2/0206/'+name+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getError(truth, noise):\n",
    "        mean_error=[]\n",
    "        for i in range(len(truth)):\n",
    "            error=sqrt((noise[i][0]-int(truth[i][0]))**2+\n",
    "                               (noise[i][1]-int(truth[i][1]))**2)\n",
    "            mean_error.append(error)\n",
    "        error_d=sum(mean_error) / float(len(mean_error))\n",
    "        return error_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5954\n",
      "161.97653081525846\n",
      "4409\n",
      "\n",
      "6911\n",
      "135.9990608392908\n",
      "4496\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getData(name):\n",
    "    import csv\n",
    "\n",
    "    tsv_data=home+'data/'+name+'f.tsv'\n",
    "\n",
    "    #Load the data from tobii\n",
    "    tobii_all_data=[]\n",
    "    with open(tsv_data) as tf:\n",
    "        treader=csv.reader(tf, delimiter='\\t')\n",
    "        for line in treader:\n",
    "            tobii_all_data.append((line[0],line[8],line[9],line[10],line[11],line[12],line[13]))\n",
    "    #The fps of Tobii is 60, so only use half of the data\n",
    "\n",
    "    tobii_data=[]\n",
    "    for i in range(1,len(tobii_all_data),2):\n",
    "        tobii_data.append(tobii_all_data[i])\n",
    "    import pickle\n",
    "    low_data= pickle.load( open( home+name+'/low_data_cut', \"rb\" ) )\n",
    "    high_data= pickle.load( open( home+name+'/high_data_cut', \"rb\" ) )\n",
    "    print(len(low_data))\n",
    "\n",
    "    #get the original accuracy\n",
    "    #store valid inputs for cnn in valid_high\n",
    "    low_coor=[]\n",
    "    high_coor=[]\n",
    "    valid_high=[] #Only keep the points when using the saliency images on the screen\n",
    "    for i in range(len(high_data)):\n",
    "        if high_data[i][1]!='' and high_data[i][1]!='phase1.jpg' and high_data[i][1]!='phase2.jpg' and high_data[i][1]!='fixation.png':\n",
    "            valid_high.append(high_data[i])\n",
    "            low_coor.append((low_data[i][1],low_data[i][2]))\n",
    "            high_coor.append((high_data[i][2],high_data[i][3]))\n",
    "    print(getError(low_coor,high_coor))\n",
    "    print(len(valid_high))\n",
    "    print()\n",
    "    return (valid_high,low_coor,high_coor)\n",
    "\n",
    "(valid_high_5, low_d_5,high_d_5)=getData('0206005')\n",
    "(valid_high_2, low_d_2,high_d_2)=getData('0206002')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the inputs for the regression contain a series of continuous eye gazes, we need to compute the continuity list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'continuity=[0] #use this list to record the length of continuous sequence for regression.\\nx=0\\nfor i in range(1,len(valid_high)):\\n    if valid_high[i][0]-valid_high[i-1][0]==1:\\n        x=x+1\\n    else:\\n        x=0\\n    continuity.append(x)\\ncnn_inputs=[] #will be used for the cnn regression model, the img index in terms of Tobii\\nfor i in range(len(continuity)-3):\\n    if continuity[i]>=2:\\n        cnn_inputs.append(valid_high[i])\\nlen(cnn_inputs)\\nvalid_high=cnn_inputs'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuity=[0] #use this list to record the length of continuous sequence for regression.\n",
    "x=0\n",
    "for i in range(1,len(valid_high)):\n",
    "    if valid_high[i][0]-valid_high[i-1][0]==1:\n",
    "        x=x+1\n",
    "    else:\n",
    "        x=0\n",
    "    continuity.append(x)\n",
    "cnn_inputs=[] #will be used for the cnn regression model, the img index in terms of Tobii\n",
    "for i in range(len(continuity)-3):\n",
    "    if continuity[i]>=2:\n",
    "        cnn_inputs.append(valid_high[i])\n",
    "len(cnn_inputs)\n",
    "valid_high=cnn_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def getTrainTEst(valid_high, name,low_coor,high_coor):\n",
    "\n",
    "def getTrainTEst(valid_high, name,low_coor,high_coor):\n",
    "    #path tp all the saliency maps of the screenshot\n",
    "    saliency_path=\"/home/gtx1080ti/Documents/hanyou2/BenchmarkIMAGES/saliency_full/\" \n",
    "    #path to the images that respresents each eye gazes\n",
    "    fixation_path=home+name+\"/calibration/gazes/\"\n",
    "    train_saliency=[]\n",
    "    train_Y=[]\n",
    "    train_g=[]\n",
    "\n",
    "    l_te=0\n",
    "    h_te=900\n",
    "    l_tr=900\n",
    "    h_tr=len(valid_high)\n",
    "\n",
    "    for i in valid_high[l_tr:h_tr]:\n",
    "        img=cv2.imread(saliency_path+i[1])\n",
    "        img=np.einsum('lij->jil',np.array(img))\n",
    "        img=[np.einsum('ij->ji',img[0])]\n",
    "        img=np.einsum('lij->ijl',np.array(img))\n",
    "        train_saliency.append(img)\n",
    "\n",
    "        train_Y.append((i[2],i[3]))\n",
    "\n",
    "        #start do multiple channels\n",
    "        img=cv2.imread(fixation_path+str(i[0])+\".jpg\")\n",
    "        img=np.einsum('lij->jil',np.array(img))\n",
    "        img=[np.einsum('ij->ji',img[0])]\n",
    "        img=np.einsum('lij->ijl',np.array(img))\n",
    "        #start do multiple channels\n",
    "        train_g.append(img)\n",
    "\n",
    "\n",
    "\n",
    "    test_saliency=[]\n",
    "    test_Y=[]\n",
    "    test_g=[]\n",
    "\n",
    "    for i in valid_high[l_te:h_te]:\n",
    "        img=cv2.imread(saliency_path+i[1])\n",
    "\n",
    "        img=np.einsum('lij->jil',np.array(img))\n",
    "        img=[np.einsum('ij->ji',img[0])]\n",
    "        img=np.einsum('lij->ijl',np.array(img))\n",
    "\n",
    "        test_saliency.append(img)\n",
    "\n",
    "        test_Y.append((i[2],i[3]))\n",
    "\n",
    "        img=cv2.imread(fixation_path+str(i[0])+\".jpg\")\n",
    "        img=np.einsum('lij->jil',np.array(img))\n",
    "        img=[np.einsum('ij->ji',img[0])]\n",
    "        img=np.einsum('lij->ijl',np.array(img))\n",
    "\n",
    "        test_g.append(img)\n",
    "        \n",
    "        \n",
    "    train_saliency=np.array(train_saliency)\n",
    "    train_Y=np.array(train_Y)\n",
    "    train_g=np.array(train_g)\n",
    "    test_saliency=np.array(test_saliency)\n",
    "    test_Y=np.array(test_Y)\n",
    "    test_g=np.array(test_g)\n",
    "    print(train_saliency.shape)\n",
    "    print(train_g.shape)\n",
    "    print(test_saliency.shape)\n",
    "    print(test_g.shape)\n",
    "\n",
    "    print(getError(low_coor,high_coor))\n",
    "    \n",
    "    train_x=np.concatenate((train_g,train_saliency),axis=3)\n",
    "    test_x=np.concatenate((test_g,test_saliency),axis=3)\n",
    "    print(train_x.shape)\n",
    "    del train_g,train_saliency,test_g,test_saliency\n",
    "    print()\n",
    "    return (train_x,train_Y,test_x,test_Y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3596, 512, 640, 1)\n",
      "(3596, 512, 640, 1)\n",
      "(900, 512, 640, 1)\n",
      "(900, 512, 640, 1)\n",
      "135.9990608392908\n",
      "(3596, 512, 640, 2)\n",
      "\n",
      "(3509, 512, 640, 1)\n",
      "(3509, 512, 640, 1)\n",
      "(900, 512, 640, 1)\n",
      "(900, 512, 640, 1)\n",
      "161.97653081525846\n",
      "(3509, 512, 640, 2)\n",
      "\n",
      "(4496, 512, 640, 2)\n",
      "(4496, 2)\n",
      "(4409, 512, 640, 2)\n",
      "(4409, 2)\n"
     ]
    }
   ],
   "source": [
    "# Produce training data, here I just use the current eye gaze instead of the recent several gazes, \n",
    "# because this tends to produce a better result\n",
    "(trainx1,trainy1,testx1,testy1)=getTrainTEst(valid_high_2,'0206002',low_d_2,high_d_2)\n",
    "\n",
    "(trainx2,trainy2,testx2,testy2)=getTrainTEst(valid_high_5,'0206005',low_d_5,high_d_5)\n",
    "del valid_high_2, low_d_2,high_d_2\n",
    "del valid_high_5, low_d_5,high_d_5\n",
    "trainx=np.concatenate((trainx1, testx1),axis=0)\n",
    "testx=np.concatenate((trainx2,testx2),axis=0)\n",
    "trainy=np.concatenate((trainy1,testy1),axis=0)\n",
    "testy=np.concatenate((trainy2,testy2),axis=0)\n",
    "print(trainx.shape)\n",
    "print(trainy.shape)\n",
    "print(testx.shape)\n",
    "print(testy.shape)\n",
    "\n",
    "del trainx2,trainx1,trainy1,trainy2\n",
    "del testx1,testx2,testy1,testy2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pickle\\ntrainx= pickle.load(open( home+name+\\'/trainx\\', \"rb\" ) )#raw error=(159.506+172.250)/2=166\\ntrainy=pickle.load(open( home+name+\\'/trainy\\', \"rb\" ) )\\ntestx= pickle.load(open( home+name+\\'/testx\\', \"rb\" ) )\\ntesty=pickle.load(open( home+name+\\'/testy\\', \"rb\" ) )\\n#import pickle\\n\\n#pickle.dump(trainx, open( home+name+\\'/trainx\\', \"wb\" ) )\\n#pickle.dump(trainy, open( home+name+\\'/trainy\\', \"wb\" ) )\\n#pickle.dump(testx, open( home+name+\\'/testx\\', \"wb\" ) )\\n#pickle.dump(testy, open( home+name+\\'/testy\\', \"wb\" ) )'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may want to store the training and testing data for using them later\n",
    "'''\n",
    "import pickle\n",
    "trainx= pickle.load(open( home+name+'/trainx', \"rb\" ) )#raw error=(159.506+172.250)/2=166\n",
    "trainy=pickle.load(open( home+name+'/trainy', \"rb\" ) )\n",
    "testx= pickle.load(open( home+name+'/testx', \"rb\" ) )\n",
    "testy=pickle.load(open( home+name+'/testy', \"rb\" ) )\n",
    "#import pickle\n",
    "\n",
    "#pickle.dump(trainx, open( home+name+'/trainx', \"wb\" ) )\n",
    "#pickle.dump(trainy, open( home+name+'/trainy', \"wb\" ) )\n",
    "#pickle.dump(testx, open( home+name+'/testx', \"wb\" ) )\n",
    "#pickle.dump(testy, open( home+name+'/testy', \"wb\" ) )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Conv2D, Flatten, Input, normalization,MaxPooling2D,concatenate\n",
    " import keras\n",
    "import tensorflow\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "ind_list=[i for i in range(len(trainx))]\n",
    "shuffle(ind_list)\n",
    "trainx=trainx[ind_list,:,:,:]\n",
    "trainy=trainy[ind_list,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gtx1080ti/.local/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=[<tf.Tenso..., outputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "input_s=Input(shape=(512,640,2))\n",
    "\n",
    "conv1_s=Conv2D(12,(9,9),strides=(3,3),activation='relu',data_format = 'channels_last')(input_s)\n",
    "\n",
    "pool1_s=MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format = 'channels_last')(conv1_s)\n",
    "\n",
    "conv2_s=Conv2D(48,(5,5),activation='relu',data_format = 'channels_last')(pool1_s)\n",
    "\n",
    "pool2_s=MaxPooling2D(pool_size=(2, 2), strides=2, padding='same', data_format = 'channels_last')(conv2_s)\n",
    "\n",
    "conv3_s=Conv2D(12,(2,2),padding='same',activation='relu',data_format = 'channels_last')(pool2_s)\n",
    "\n",
    "pool3_s=MaxPooling2D(pool_size=(2, 2), strides=2, padding='valid', data_format = 'channels_last')(conv3_s)\n",
    "\n",
    "#norm1_s=keras.layers.BatchNormalization(axis=2, momentum=0.99, epsilon=0.001, center=True, scale=True)(pool1_s)\n",
    "\n",
    "flat1_s=Flatten()(pool3_s)\n",
    "\n",
    "dense1_s=Dense(units=64,activation='relu')(flat1_s)\n",
    "\n",
    "\n",
    "dense2_s=Dense(units=16,activation='relu')(dense1_s)\n",
    "\n",
    "\n",
    "dense2=Dense(units=2)(dense2_s)\n",
    "\n",
    "\n",
    "model=Model(inputs=[input_s],output=[dense2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "        optimizer=keras.optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=0.0001, decay=0.000), \n",
    "        #optimizer=keras.optimizers.SGD(lr=0.003, momentum=0.9, decay=0.00001, nesterov=True),\n",
    "        metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "4496/4496 [==============================] - 8s 2ms/step - loss: 143308.7818 - acc: 0.6428\n",
      "367.6597982675373\n",
      "298.5920264718227\n",
      "Epoch 1/1\n",
      "4496/4496 [==============================] - 8s 2ms/step - loss: 25037.0547 - acc: 0.7983\n",
      "168.05233157609052\n",
      "121.81855451375833\n",
      "Epoch 1/1\n",
      "4496/4496 [==============================] - 8s 2ms/step - loss: 10194.3059 - acc: 0.8921\n",
      "150.74889021956878\n",
      "97.71949965106819\n",
      "Epoch 1/1\n",
      "4496/4496 [==============================] - 8s 2ms/step - loss: 8533.7397 - acc: 0.8939\n",
      "144.290004543273\n",
      "90.6017250014552\n",
      "Epoch 1/1\n",
      "4496/4496 [==============================] - 8s 2ms/step - loss: 7837.7192 - acc: 0.8977\n",
      "141.52756386111\n",
      "86.94261149471463\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    history = model.fit([trainx], [trainy], epochs=1, batch_size=64, \n",
    "             verbose=True)\n",
    "\n",
    "    predY=model.predict([testx])\n",
    "    tmp=getError(predY,testy)\n",
    "    print(tmp)\n",
    "\n",
    "    predY=model.predict([trainx])\n",
    "    tmp=getError(predY,trainy)\n",
    "    print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#model.save(home+name+'/cnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
